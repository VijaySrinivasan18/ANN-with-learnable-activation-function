{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Importing Necessary Library\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from random import random,seed\n",
    "from random import randrange\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,cohen_kappa_score\n",
    "from math import exp\n",
    "from scipy.special import softmax\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"dataset/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def loadCsv(filename):\n",
    "        trainset=pd.read_csv(filename)\n",
    "        return trainset\n",
    "\n",
    "# Normalization\n",
    "\n",
    "def normalize(dataset):\n",
    "    min_max_obj=MinMaxScaler()\n",
    "    transformed_data=min_max_obj.fit_transform(dataset)\n",
    "    return transformed_data\n",
    "\n",
    "# Convert column values to float\n",
    "def column_to_float(dataset, column):\n",
    "        return dataset[column].astype(float)\n",
    "\n",
    "#  Label encoding target variable\n",
    "\n",
    "def encode_target(dataset,target_column):\n",
    "        label_encoder=LabelEncoder()\n",
    "        label_encoded=label_encoder.fit_transform(dataset.iloc[:,target_column])\n",
    "        with open(\"artifacts/label_encoder.pickle\",\"wb\") as s:\n",
    "                pickle.dump(label_encoder,s)\n",
    "        print(\"Variable encoded\")\n",
    "        return label_encoded\n",
    "\n",
    "# Method to create cross validation splits\n",
    "\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "        dataset_split = list()\n",
    "        dataset_copy = dataset.to_numpy().tolist()\n",
    "        fold_size = int(len(dataset_copy) / n_folds)\n",
    "        for i in range(n_folds):\n",
    "                fold = list()\n",
    "                while len(fold) < fold_size:\n",
    "                        index = randrange(len(dataset_copy))\n",
    "                        fold.append(dataset_copy.pop(index))\n",
    "                dataset_split.append(fold)\n",
    "        print(\"Cross validation split done\")\n",
    "        return dataset_split\n",
    "\n",
    "#  Method to Create a neural network\n",
    "\n",
    "# Creating Neural network with 1 hidden layes\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "        network = list()\n",
    "        hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)], 'prev':[0 for i in range(n_inputs+1)]} for i in range(n_hidden)]        \n",
    "        network.append(hidden_layer)\n",
    "        output_layer = [{'weights':[random() for i in range(n_hidden + 1)],'prev':[0 for i in range(n_hidden+1)]} for i in range(n_outputs)]\n",
    "        network.append(output_layer)\n",
    "        # initialize parameters of activation function g(x) = k0 + k1x\n",
    "        k=[random(),random()]\n",
    "        return network,k\n",
    "\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "        # print(\"Activate func\")\n",
    "        # print(weights)\n",
    "        # print(inputs)\n",
    "        activation = weights[-1]   # Weights[-1]  is Bias term\n",
    "        for i in range(len(weights)-1):\n",
    "                activation += weights[i] * inputs[i]\n",
    "        return activation\n",
    "\n",
    "\n",
    "# Transfer neuron activation     g(x) = k0 + k1x\n",
    "def transfer(activation,K):\n",
    "        return K[0]+K[1]*(activation)\n",
    "\n",
    "\n",
    "# Forward propagate input to a network output  -  Hidden Layers use Ada Acti Activation (g(x) = k0 + k1x) and output layer use Softmax\n",
    "def forward_propagate(network, row,K):\n",
    "        inputs = row\n",
    "        val={}\n",
    "        for index,layer in enumerate(network):\n",
    "                if index!=len(network)-1:\n",
    "                        new_inputs = []\n",
    "                        z=[]\n",
    "                        for neuron in layer:\n",
    "                                activation = activate(neuron['weights'], inputs)\n",
    "                                z.append(activation)\n",
    "                                # print(f\"Hidden Layer {index} ,Applying Ada Act Activation\")\n",
    "                                neuron['output'] = transfer(activation,K)\n",
    "                                new_inputs.append(neuron['output'])\n",
    "                        val[f\"Z{index+1}\"]=np.array(z)\n",
    "                        val[f\"A{index+1}\"]=np.array(new_inputs)\n",
    "                        inputs = new_inputs\n",
    "                else:\n",
    "                        # print(\"Final output layer : Applying Softmax\")\n",
    "                        z=[]\n",
    "                        new_inputs = []\n",
    "                        for neuron in layer:\n",
    "                                activation = activate(neuron['weights'], inputs)\n",
    "                                z.append(activation)\n",
    "                                neuron['output'] = activation\n",
    "                                new_inputs.append(neuron['output'])\n",
    "                        val[f\"Z{index+1}\"]=np.array(z)\n",
    "                        inputs = softmax(new_inputs,axis=0)\n",
    "                        val[f\"A{index+1}\"]=np.array(inputs)   \n",
    "                        \n",
    "        return inputs,val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network,X_row,expected,val,yhat,K):\n",
    "        # print(\"Back propagate Error started\")\n",
    "        derivatives={}\n",
    "        params={}\n",
    "        W2=[]   #  weights in final layer                                                        # 3x3\n",
    "        for i in network[1]:\n",
    "                W2.append(i[\"weights\"][:-1])\n",
    "\n",
    "        W1=[]   #  weights in 1st hidden layer  layer                                            # 3x3\n",
    "        for i in network[0] : # weights in first hiddent layer\n",
    "                W1.append(i[\"weights\"][:-1])\n",
    "\n",
    "        B2=[]   #  Bias in final layer                                                           #1x3\n",
    "        for i in network[1]:\n",
    "                B2.append(i[\"weights\"][-1])\n",
    "\n",
    "        B1=[]    #  Bias in hidden  layer                                                         #1x3\n",
    "        for i in network[0] : # weights in first hiddent layer\n",
    "                B1.append(i[\"weights\"][-1])\n",
    "\n",
    "        W2=np.array(W2)\n",
    "        W1=np.array(W1)\n",
    "        B2=np.array(B2)\n",
    "        B1=np.array(B1)\n",
    "        K=np.array(K)\n",
    "\n",
    "\n",
    "        params[\"W2\"]=W2\n",
    "        params[\"W1\"]=W1\n",
    "        params[\"B2\"]=B2\n",
    "        params[\"B1\"]=B1\n",
    "        params[\"K\"]=K\n",
    "\n",
    "        \n",
    "        # dz3 = a3 − y  where a3 is output from final layer after softmax activation\n",
    "        dZ2=  np.expand_dims(np.array(expected)-np.array(yhat),axis=0)\n",
    "                                                                      #1x3\n",
    "        #  dw3 = (1/t) aT2 × dz3\n",
    "        dW2=np.dot(np.expand_dims(val[\"A1\"],axis=0).T,dZ2)                                                       #3x3\n",
    "\n",
    "\n",
    "        # db3 = avgcol(dz3)\n",
    "\n",
    "        dB2=np.mean(dZ2,axis=0)\n",
    "\n",
    "        # da1 = dz2 × w2T      w2 is the weights in final layer\n",
    "        dA1=np.dot(dZ2,np.array(W2))                                                                \n",
    "\n",
    "        # dz1 = g' (z1) ∗ da1\n",
    "\n",
    "\n",
    "        dZ1= np.array(K[1]*dA1)\n",
    "\n",
    "        # dw1 = 1/t a0.T × dz1  a0 is the input given to neural  network\n",
    "        dW1=np.dot(dZ1.T,np.expand_dims(np.array(X_row),axis=1))\n",
    "\n",
    "        #  db1 = avgcol(dz1)\n",
    "        dB1=np.mean(dZ1,axis=1)\n",
    "        # print(\"dB1 shape\",dB1.shape)\n",
    "\n",
    "\n",
    "        #  Calculating K derivatives\n",
    "\n",
    "        dK=[np.mean(dA1)]\n",
    "        dK.append(np.mean(np.multiply(dA1,val[\"Z1\"])))\n",
    "        dk=np.array(dK)\n",
    "        # print(\"dK\",dK)\n",
    "        derivatives[\"dW1\"]=dW1\n",
    "        derivatives[\"dW2\"]=dW2\n",
    "        derivatives[\"dB1\"]=dB1\n",
    "        derivatives[\"dB2\"]=dB2\n",
    "        derivatives[\"dK\"]=dK\n",
    "\n",
    "        return derivatives,params\n",
    "\n",
    "\n",
    "def update_params(network,alpha, derivatives, params):\n",
    "    W1 = params[\"W1\"]\n",
    "    W2 = params[\"W2\"]\n",
    "    B1 = params[\"B1\"]\n",
    "    B2 = params[\"B2\"]\n",
    "    K = params[\"K\"]\n",
    "    \n",
    "    dW1 = derivatives[\"dW1\"]\n",
    "    dW2 = derivatives[\"dW2\"]\n",
    "    dB1 = derivatives[\"dB1\"]\n",
    "    dB2 = derivatives[\"dB2\"]\n",
    "    dK = derivatives[\"dK\"]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    params[\"W1\"] = W1-(alpha*(dW1))\n",
    "    params[\"W2\"] = W2-(alpha*(dW2))\n",
    "    \n",
    "    params[\"B1\"] = B1-(alpha*(dB1))\n",
    "    params[\"B2\"] = B2-(alpha*(dB2))\n",
    "    params[\"K\"] = K-(alpha*np.array(dK))\n",
    "\n",
    "    new_hl_weights=np.concatenate((params[\"W1\"],params[\"B1\"].reshape(3,-1)),axis=1).tolist()\n",
    "    new_h2_weights=np.concatenate((params[\"W2\"],params[\"B2\"].reshape(3,-1)),axis=1).tolist()\n",
    "\n",
    "    for ac_layer,up_layer in zip(network,(new_hl_weights,new_h2_weights)):\n",
    "        for neuron,new_weights in zip(ac_layer,up_layer):\n",
    "                neuron[\"weights\"]=new_weights\n",
    "\n",
    "    return params\n",
    "                \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs,K,fold):\n",
    "        target_col=train.columns[-1]\n",
    "        acc_epoch={}\n",
    "        for epoch in range(n_epoch):\n",
    "                act=[]\n",
    "                out=[]\n",
    "                for index in train.index:\n",
    "                        row=train.loc[index,:]\n",
    "                        outputs,val = forward_propagate(network, row,K)\n",
    "                        #print(network)\n",
    "                        expected = [0 for i in range(n_outputs)]\n",
    "                        # print(\"Row\",row[-1])\n",
    "                        # print(\"Expected\",expected)\n",
    "                        expected[int(row[-1])] = 1\n",
    "                        #print(\"expected row{}\\n\".format(expected))\n",
    "                        derivatives,params= backward_propagate_error(network,train.loc[[index],target_col],expected,val,outputs,K)\n",
    "                        # print(f\"Epoch {epoch}\")\n",
    "                        params=update_params(network,l_rate,derivatives,params)\n",
    "                        K=params[\"K\"]\n",
    "                        act.append(int(train.loc[index,train.columns[-1]]))\n",
    "                        out.append(predict(network,row,K))\n",
    "                acc_epoch[f\"epoch{epoch}\"]=accuracy_score(act,out)\n",
    "\n",
    "        acc_epoch=pd.DataFrame(acc_epoch,index=[0])\n",
    "        acc_epoch.to_csv(f\"artifacts/acc_fold_{fold}.csv\",index=False)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "        return K\n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row,k):\n",
    "        outputs,_ = forward_propagate(network, row,k)\n",
    "        # print(outputs)\n",
    "        return np.argmax(outputs)\n",
    "\n",
    "def loss(Y_hat, Y):\n",
    "    N = len(Y)\n",
    "    ce = -np.sum(np.multiply(Y, np.log(Y_hat))) / N\n",
    "    return ce\n",
    "\n",
    "\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden,fold):\n",
    "        n_inputs = len(train.columns) - 1\n",
    "        n_outputs = len(train[train.columns[-1]].unique())\n",
    "        network,K = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "\n",
    "        print(\"initial K\",K)\n",
    "        K=train_network(network, train, l_rate, n_epoch, n_outputs,K,fold)\n",
    "        #print(\"network {}\\n\".format(network))\n",
    "        predictions = list()\n",
    "        for row in test.index:\n",
    "                prediction = predict(network, test.loc[row,:],K)\n",
    "                predictions.append(prediction)\n",
    "        print(\"Final K\",K)\n",
    "        # vars()[f\"Final_k_{fold}\"]=K\n",
    "        return(predictions,K,network)\n",
    "\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def run_algorithm(dataset, algorithm, n_folds, l_rate, n_epoch, n_hidden):\n",
    "        \n",
    "        folds = cross_validation_split(dataset, n_folds)\n",
    "        #for fold in folds:\n",
    "                #print(\"Fold {} \\n \\n\".format(fold))\n",
    "        acc_scores = {}\n",
    "        prec_score={}\n",
    "        rec_score={}\n",
    "        specificity={}\n",
    "        sensitivity={}\n",
    "        cohen_kappa={}\n",
    "        # loss_val={}\n",
    "        for index,fold in enumerate(folds):\n",
    "                #print(\"Test Fold {} \\n \\n\".format(fold))\n",
    "                train_set = list(folds)\n",
    "                train_set.remove(fold)\n",
    "                train_set = sum(train_set, [])\n",
    "                test_set = list()\n",
    "                for row in fold:\n",
    "                        row_copy = list(row)\n",
    "                        test_set.append(row_copy)\n",
    "                        # row_copy[-1] = None\n",
    "                train_set=pd.DataFrame(train_set,columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
    "       'species'])\n",
    "                test_set=pd.DataFrame(test_set,columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
    "       'species'])\n",
    "                # print(\"Gonna start back prop algo\")\n",
    "                predicted,K,network = algorithm(train_set, test_set,l_rate, n_epoch, n_hidden,index)\n",
    "                # print(\"back propagation algo done\")\n",
    "                actual = np.array([row[-1] for row in fold])\n",
    "                \n",
    "                accuracy = accuracy_score(actual, predicted)\n",
    "\n",
    "                cm = confusion_matrix(actual, predicted)\n",
    "                print('\\n'.join([''.join(['{:4}'.format(item) for item in row]) for row in cm]))\n",
    "                #confusionmatrix = np.matrix(cm)\n",
    "                FP = cm.sum(axis=0) - np.diag(cm)\n",
    "                FN = cm.sum(axis=1) - np.diag(cm)\n",
    "                TP = np.diag(cm)\n",
    "                TN = cm.sum() - (FP + FN + TP)\n",
    "                print('False Positives\\n {}'.format(FP))\n",
    "                print('False Negetives\\n {}'.format(FN))\n",
    "                print('True Positives\\n {}'.format(TP))\n",
    "                print('True Negetives\\n {}'.format(TN))\n",
    "                TPR = TP/(TP+FN)\n",
    "                print('Sensitivity \\n {}'.format(TPR))\n",
    "                TNR = TN/(TN+FP)\n",
    "                print('Specificity \\n {}'.format(TNR))\n",
    "                Precision = TP/(TP+FP)\n",
    "                print('Precision \\n {}'.format(Precision))\n",
    "                Recall = TP/(TP+FN)\n",
    "                print('Recall \\n {}'.format(Recall))\n",
    "                Acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "                print('Áccuracy \\n{}'.format(Acc))\n",
    "                Fscore = 2*(Precision*Recall)/(Precision+Recall)\n",
    "                print('FScore \\n{}'.format(Fscore))\n",
    "                coh=cohen_kappa_score(actual, predicted)\n",
    "                print('Çohen Kappa \\n{}'.format(coh))\n",
    "\n",
    "                acc_scores[f\"fold{index}\"]=accuracy\n",
    "                prec_score[f\"fold{index}\"]=Precision\n",
    "                rec_score[f\"fold{index}\"]=Recall\n",
    "                specificity[f\"fold{index}\"]=TNR\n",
    "                sensitivity[f\"fold{index}\"]=TPR\n",
    "                cohen_kappa[f\"fold{index}\"]=coh\n",
    "\n",
    "        return acc_scores,prec_score,rec_score,specificity,sensitivity,cohen_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable encoded\n",
      "Cross validation split done\n",
      "initial K [0.18803930475131292, 0.10876169244541334]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_14828\\3506005137.py:63: RuntimeWarning: overflow encountered in double_scalars\n",
      "  activation += weights[i] * inputs[i]\n",
      "c:\\Users\\Vijay\\anaconda3\\envs\\dl\\lib\\site-packages\\scipy\\special\\_logsumexp.py:214: RuntimeWarning: invalid value encountered in subtract\n",
      "  return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final K [nan nan]\n",
      "  11   0   0\n",
      "   8   0   0\n",
      "  11   0   0\n",
      "False Positives\n",
      " [19  0  0]\n",
      "False Negetives\n",
      " [ 0  8 11]\n",
      "True Positives\n",
      " [11  0  0]\n",
      "True Negetives\n",
      " [ 0 22 19]\n",
      "Sensitivity \n",
      " [1. 0. 0.]\n",
      "Specificity \n",
      " [0. 1. 1.]\n",
      "Precision \n",
      " [0.36666667        nan        nan]\n",
      "Recall \n",
      " [1. 0. 0.]\n",
      "Áccuracy \n",
      "[0.36666667 0.73333333 0.63333333]\n",
      "FScore \n",
      "[0.53658537        nan        nan]\n",
      "Çohen Kappa \n",
      "0.0\n",
      "initial K [0.30638662033324593, 0.8585144063565593]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_14828\\3506005137.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Precision = TP/(TP+FP)\n",
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_14828\\3506005137.py:69: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return K[0]+K[1]*(activation)\n",
      "c:\\Users\\Vijay\\anaconda3\\envs\\dl\\lib\\site-packages\\scipy\\special\\_logsumexp.py:214: RuntimeWarning: invalid value encountered in subtract\n",
      "  return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final K [nan nan]\n",
      "  10   0   0\n",
      "  11   0   0\n",
      "   9   0   0\n",
      "False Positives\n",
      " [20  0  0]\n",
      "False Negetives\n",
      " [ 0 11  9]\n",
      "True Positives\n",
      " [10  0  0]\n",
      "True Negetives\n",
      " [ 0 19 21]\n",
      "Sensitivity \n",
      " [1. 0. 0.]\n",
      "Specificity \n",
      " [0. 1. 1.]\n",
      "Precision \n",
      " [0.33333333        nan        nan]\n",
      "Recall \n",
      " [1. 0. 0.]\n",
      "Áccuracy \n",
      "[0.33333333 0.63333333 0.7       ]\n",
      "FScore \n",
      "[0.5 nan nan]\n",
      "Çohen Kappa \n",
      "0.0\n",
      "initial K [0.8716215074235552, 0.8996782696347811]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_14828\\3506005137.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Precision = TP/(TP+FP)\n",
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_14828\\3506005137.py:63: RuntimeWarning: overflow encountered in double_scalars\n",
      "  activation += weights[i] * inputs[i]\n",
      "c:\\Users\\Vijay\\anaconda3\\envs\\dl\\lib\\site-packages\\scipy\\special\\_logsumexp.py:214: RuntimeWarning: invalid value encountered in subtract\n",
      "  return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final K [nan nan]\n",
      "   6   0   0\n",
      "  10   0   0\n",
      "  14   0   0\n",
      "False Positives\n",
      " [24  0  0]\n",
      "False Negetives\n",
      " [ 0 10 14]\n",
      "True Positives\n",
      " [6 0 0]\n",
      "True Negetives\n",
      " [ 0 20 16]\n",
      "Sensitivity \n",
      " [1. 0. 0.]\n",
      "Specificity \n",
      " [0. 1. 1.]\n",
      "Precision \n",
      " [0.2 nan nan]\n",
      "Recall \n",
      " [1. 0. 0.]\n",
      "Áccuracy \n",
      "[0.2        0.66666667 0.53333333]\n",
      "FScore \n",
      "[0.33333333        nan        nan]\n",
      "Çohen Kappa \n",
      "0.0\n",
      "initial K [0.8674198235869027, 0.6039825288917112]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_14828\\3506005137.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Precision = TP/(TP+FP)\n",
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_14828\\3506005137.py:63: RuntimeWarning: overflow encountered in double_scalars\n",
      "  activation += weights[i] * inputs[i]\n",
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_14828\\3506005137.py:63: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  activation += weights[i] * inputs[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final K [nan nan]\n",
      "  10   0   0\n",
      "  10   0   0\n",
      "  10   0   0\n",
      "False Positives\n",
      " [20  0  0]\n",
      "False Negetives\n",
      " [ 0 10 10]\n",
      "True Positives\n",
      " [10  0  0]\n",
      "True Negetives\n",
      " [ 0 20 20]\n",
      "Sensitivity \n",
      " [1. 0. 0.]\n",
      "Specificity \n",
      " [0. 1. 1.]\n",
      "Precision \n",
      " [0.33333333        nan        nan]\n",
      "Recall \n",
      " [1. 0. 0.]\n",
      "Áccuracy \n",
      "[0.33333333 0.66666667 0.66666667]\n",
      "FScore \n",
      "[0.5 nan nan]\n",
      "Çohen Kappa \n",
      "0.0\n",
      "initial K [0.09168312261651779, 0.1151024984279273]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_14828\\3506005137.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Precision = TP/(TP+FP)\n",
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_14828\\3506005137.py:69: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return K[0]+K[1]*(activation)\n",
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_14828\\3506005137.py:63: RuntimeWarning: overflow encountered in double_scalars\n",
      "  activation += weights[i] * inputs[i]\n",
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_14828\\3506005137.py:63: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  activation += weights[i] * inputs[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final K [nan nan]\n",
      "  13   0   0\n",
      "  11   0   0\n",
      "   6   0   0\n",
      "False Positives\n",
      " [17  0  0]\n",
      "False Negetives\n",
      " [ 0 11  6]\n",
      "True Positives\n",
      " [13  0  0]\n",
      "True Negetives\n",
      " [ 0 19 24]\n",
      "Sensitivity \n",
      " [1. 0. 0.]\n",
      "Specificity \n",
      " [0. 1. 1.]\n",
      "Precision \n",
      " [0.43333333        nan        nan]\n",
      "Recall \n",
      " [1. 0. 0.]\n",
      "Áccuracy \n",
      "[0.43333333 0.63333333 0.8       ]\n",
      "FScore \n",
      "[0.60465116        nan        nan]\n",
      "Çohen Kappa \n",
      "0.0\n",
      "Scores: {'fold0': 0.36666666666666664, 'fold1': 0.3333333333333333, 'fold2': 0.2, 'fold3': 0.3333333333333333, 'fold4': 0.43333333333333335}\n",
      "Mean Accuracy: 0.333%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vijay\\AppData\\Local\\Temp\\ipykernel_14828\\3506005137.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Precision = TP/(TP+FP)\n"
     ]
    }
   ],
   "source": [
    "seed(1)\n",
    "filename = 'dataset/iris.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "#  convert independent variables to float\n",
    "for i in df.columns[:-1]:\n",
    "        df[i]=column_to_float(df,i)\n",
    "\n",
    "# Label Encoding target variable\n",
    "df[\"species\"]=encode_target(df,4)\n",
    "\n",
    "# Normalize the independent variable\n",
    "\n",
    "df.iloc[:,:4]=normalize(df.iloc[:,:4])\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.3\n",
    "n_epoch = 30\n",
    "n_hidden = 3\n",
    "acc_scores,prec_score,rec_score,specificity,sensitivity,cohen_kappa = run_algorithm(df, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "\n",
    "# print(acc_scores)\n",
    "\n",
    "print('Scores: %s' % acc_scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(acc_scores.values())/float(len(acc_scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fold0': 0.36666666666666664,\n",
       " 'fold1': 0.3333333333333333,\n",
       " 'fold2': 0.2,\n",
       " 'fold3': 0.3333333333333333,\n",
       " 'fold4': 0.43333333333333335}"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean accuracy scores for all 5 folds for 30 epochs\")\n",
    "print(acc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31b4864f7a329bb7df33ee910d06966add06d4194ca7784060c0c775d42f1d3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
